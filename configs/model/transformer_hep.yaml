block: model.transformer_hep
params:
  sequence_length: 50 # Max particles per event/jet
  input_dim: 4 # 4-momentum features
  d_model: 128 # Model dimension
  nhead: 8 # Number of attention heads
  num_layers: 4 # Number of transformer layers
  dim_feedforward: 256
  dropout: 0.1
  output_dim: 2
  learning_rate: 0.001
  batch_size: 32
  max_epochs: 100
  normalize_inputs: true
  use_positional_encoding: true
